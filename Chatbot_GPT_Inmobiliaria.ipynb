{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Chatbot para inmobiliaria utilizando GPT\n",
        "\n",
        "Fecha: 28/04/2023\n",
        "\n",
        "## Objetivo:\n",
        "\n",
        "Crear un chatbot que permita responder preguntas a partir de un archivo en formato PDF. El archivo debe de tener texto que permita al chatbot responder preguntas realizadas por un usuario.\n",
        "\n",
        "El chatbot estará entrenado con datos relacionados al rubro de inmobiliaria.\n",
        "\n",
        "## Procedimiento:\n",
        "\n",
        "Considerar la secuencia de pasos indicados.\n",
        "\n",
        "Los archivos PDF deberán ubicarse en la carpeta o directorio `/dataset`. Debajo en el código aparece la línea de comando para crear esta carpeta.\n",
        "\n",
        "\n",
        "\n",
        "## Referencias:\n",
        "\n",
        "- Word embedding [https://youtu.be/-XVkdIdli0I](https://youtu.be/-XVkdIdli0I)\n",
        "\n",
        "- Cómo Usar ChatGPT Sobre Información Interna De Tu Negocio [https://youtu.be/hVXpAh1FQCQ?t=725](https://youtu.be/hVXpAh1FQCQ?t=725)\n"
      ],
      "metadata": {
        "id": "UbEPG_flaJUD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Paso 0: Conectar a Google Drive\n",
        "\n"
      ],
      "metadata": {
        "id": "2Aye6lmW9LvI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Conexión con Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eBgjL0Mn9ZIS",
        "outputId": "3efe0171-9b8a-4a5f-ee1c-8e583d6326ee"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Paso 1: Configuración e instalación de librerías"
      ],
      "metadata": {
        "id": "oCYY5CDvag9n"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Instalar librerías"
      ],
      "metadata": {
        "id": "E-pkdmQ9cyH1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- langchain: Interactuar con LLMs\n",
        "- llama-index: Chunks e indexar"
      ],
      "metadata": {
        "id": "vSon5z0Pcz-b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install PyPDF2 openai langchain llama-index==0.5.25"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nioo2mbT9ZNF",
        "outputId": "f6a3f2d3-624b-4815-859c-cef52b3faccc"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting PyPDF2\n",
            "  Downloading pypdf2-3.0.1-py3-none-any.whl (232 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m232.6/232.6 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting openai\n",
            "  Downloading openai-0.27.6-py3-none-any.whl (71 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.9/71.9 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting langchain\n",
            "  Downloading langchain-0.0.163-py3-none-any.whl (781 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m782.0/782.0 kB\u001b[0m \u001b[31m26.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting llama-index==0.5.25\n",
            "  Downloading llama_index-0.5.25.tar.gz (188 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m188.5/188.5 kB\u001b[0m \u001b[31m19.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting dataclasses_json (from llama-index==0.5.25)\n",
            "  Downloading dataclasses_json-0.5.7-py3-none-any.whl (25 kB)\n",
            "Collecting langchain\n",
            "  Downloading langchain-0.0.142-py3-none-any.whl (548 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m548.8/548.8 kB\u001b[0m \u001b[31m42.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from llama-index==0.5.25) (1.22.4)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.2.0 in /usr/local/lib/python3.10/dist-packages (from llama-index==0.5.25) (8.2.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from llama-index==0.5.25) (1.5.3)\n",
            "Collecting tiktoken (from llama-index==0.5.25)\n",
            "  Downloading tiktoken-0.4.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m59.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: PyYAML>=5.4.1 in /usr/local/lib/python3.10/dist-packages (from langchain) (6.0)\n",
            "Collecting SQLAlchemy<2,>=1 (from langchain)\n",
            "  Downloading SQLAlchemy-1.4.48-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m61.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting aiohttp<4.0.0,>=3.8.3 (from langchain)\n",
            "  Downloading aiohttp-3.8.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m63.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting async-timeout<5.0.0,>=4.0.0 (from langchain)\n",
            "  Downloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n",
            "Collecting gptcache>=0.1.7 (from langchain)\n",
            "  Downloading gptcache-0.1.22-py3-none-any.whl (82 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m82.2/82.2 kB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numexpr<3.0.0,>=2.8.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.8.4)\n",
            "Collecting openapi-schema-pydantic<2.0,>=1.2 (from langchain)\n",
            "  Downloading openapi_schema_pydantic-1.2.4-py3-none-any.whl (90 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m90.0/90.0 kB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pydantic<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.10.7)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.27.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai) (4.65.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (2.0.12)\n",
            "Collecting multidict<7.0,>=4.5 (from aiohttp<4.0.0,>=3.8.3->langchain)\n",
            "  Downloading multidict-6.0.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (114 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.5/114.5 kB\u001b[0m \u001b[31m14.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting yarl<2.0,>=1.0 (from aiohttp<4.0.0,>=3.8.3->langchain)\n",
            "  Downloading yarl-1.9.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (268 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m29.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting frozenlist>=1.1.1 (from aiohttp<4.0.0,>=3.8.3->langchain)\n",
            "  Downloading frozenlist-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (149 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m149.6/149.6 kB\u001b[0m \u001b[31m17.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting aiosignal>=1.1.2 (from aiohttp<4.0.0,>=3.8.3->langchain)\n",
            "  Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
            "Collecting marshmallow<4.0.0,>=3.3.0 (from dataclasses_json->llama-index==0.5.25)\n",
            "  Downloading marshmallow-3.19.0-py3-none-any.whl (49 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.1/49.1 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting marshmallow-enum<2.0.0,>=1.5.1 (from dataclasses_json->llama-index==0.5.25)\n",
            "  Downloading marshmallow_enum-1.5.1-py2.py3-none-any.whl (4.2 kB)\n",
            "Collecting typing-inspect>=0.4.0 (from dataclasses_json->llama-index==0.5.25)\n",
            "  Downloading typing_inspect-0.8.0-py3-none-any.whl (8.7 kB)\n",
            "Requirement already satisfied: cachetools in /usr/local/lib/python3.10/dist-packages (from gptcache>=0.1.7->langchain) (5.3.0)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<2,>=1->langchain) (4.5.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2022.12.7)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.4)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<2,>=1->langchain) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->llama-index==0.5.25) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->llama-index==0.5.25) (2022.7.1)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken->llama-index==0.5.25) (2022.10.31)\n",
            "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.10/dist-packages (from marshmallow<4.0.0,>=3.3.0->dataclasses_json->llama-index==0.5.25) (23.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->llama-index==0.5.25) (1.16.0)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect>=0.4.0->dataclasses_json->llama-index==0.5.25)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Building wheels for collected packages: llama-index\n",
            "  Building wheel for llama-index (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for llama-index: filename=llama_index-0.5.25-py3-none-any.whl size=281245 sha256=d8d74b6a8872aa4b0f32fbc799d51088d2212a0f7c8a404a5d2db4809ce9438b\n",
            "  Stored in directory: /root/.cache/pip/wheels/63/3c/35/e4fc52bd465241a1e257d411ce02f93b7b2e0f8442d813cf2e\n",
            "Successfully built llama-index\n",
            "Installing collected packages: SQLAlchemy, PyPDF2, mypy-extensions, multidict, marshmallow, frozenlist, async-timeout, yarl, typing-inspect, tiktoken, openapi-schema-pydantic, marshmallow-enum, aiosignal, dataclasses_json, aiohttp, openai, gptcache, langchain, llama-index\n",
            "  Attempting uninstall: SQLAlchemy\n",
            "    Found existing installation: SQLAlchemy 2.0.10\n",
            "    Uninstalling SQLAlchemy-2.0.10:\n",
            "      Successfully uninstalled SQLAlchemy-2.0.10\n",
            "Successfully installed PyPDF2-3.0.1 SQLAlchemy-1.4.48 aiohttp-3.8.4 aiosignal-1.3.1 async-timeout-4.0.2 dataclasses_json-0.5.7 frozenlist-1.3.3 gptcache-0.1.22 langchain-0.0.142 llama-index-0.5.25 marshmallow-3.19.0 marshmallow-enum-1.5.1 multidict-6.0.4 mypy-extensions-1.0.0 openai-0.27.6 openapi-schema-pydantic-1.2.4 tiktoken-0.4.0 typing-inspect-0.8.0 yarl-1.9.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Importar librerías"
      ],
      "metadata": {
        "id": "ZzuwmP7ldkf0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "CLwbbDlN88YY"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import openai\n",
        "from llama_index import GPTSimpleVectorIndex, SimpleDirectoryReader, LLMPredictor, ServiceContext\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "import textwrap\n",
        "\n",
        "# Procesar datos de PDF\n",
        "from langchain.document_loaders import PyPDFLoader\n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "\n",
        "#import gradio as gr\n",
        "from openai.embeddings_utils import get_embedding\n",
        "from openai.embeddings_utils import cosine_similarity"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Clases"
      ],
      "metadata": {
        "id": "l5KFkytqefBL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Clase ChatBotInmobiliaria"
      ],
      "metadata": {
        "id": "Y-3xT2e5ea7X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ChatBotInmobiliaria():\n",
        "    def __init__(self):\n",
        "        self.embedding_engine = \"text-embedding-ada-002\"\n",
        "        self.model_name = \"gpt-3.5-turbo\"\n",
        "        self.index = None\n",
        "    \n",
        "    def create_dataset(self, directory_path, filepath_dataset):\n",
        "        # directory_path: Directorio donde se ubican los archivos PDF.\n",
        "        # filepath_dataset: Nombre del archivo JSON vectorizado.\n",
        "        if directory_path != None:\n",
        "            #Leer los PDFs\n",
        "            pdf = SimpleDirectoryReader(directory_path).load_data()\n",
        "            #Definir e instanciar el modelo\n",
        "            modelo = LLMPredictor(llm=ChatOpenAI(temperature=0, model_name=self.model_name))\n",
        "            #Indexar el contenido de los PDFs\n",
        "            service_context = ServiceContext.from_defaults(llm_predictor=modelo)\n",
        "            self.index = GPTSimpleVectorIndex.from_documents(pdf, service_context = service_context)\n",
        "            self.__save_model(filepath_dataset)\n",
        "\n",
        "    def __save_model(self, filepath):\n",
        "        #Guardar el índice a disco para no tener que repetir cada vez\n",
        "        #Recordar que necesistaríamos persistir el drive para que lo mantenga\n",
        "        self.index.save_to_disk(filepath)\n",
        "    \n",
        "    def load_dataset(self, filepath):\n",
        "        #Cargar el índice del disco\n",
        "        self.index = GPTSimpleVectorIndex.load_from_disk(filepath)\n",
        "\n",
        "    def ask(self, question=\"\"):\n",
        "        if len(question) == 0:\n",
        "            print(\"Debe de ingresar una pregunta.\")\n",
        "        try:\n",
        "            respuesta = self.index.query(question)\n",
        "            for frase in textwrap.wrap(respuesta.response, width=100):\n",
        "                print(frase)\n",
        "        except Exception as e:\n",
        "            print(e)"
      ],
      "metadata": {
        "id": "igGrocq6fP0l"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Paso 2: Configurar variables"
      ],
      "metadata": {
        "id": "tSvZgk-Xdp0H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Crear la carpeta datos y mover los archivos PDF a esa carpeta\n",
        "\n",
        "dataset_directory = \"/content/drive/MyDrive/dataset\"\n",
        "\n",
        "if os.path.exists(dataset_directory) == False:\n",
        "    os.mkdir(dataset_directory)"
      ],
      "metadata": {
        "id": "bpWNY13w44U3"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# API de OpenAI\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"sk-\"\n",
        "\n",
        "DATASET_INMOBILIARIA_JSON = \"demo-inmobiliaria.json\""
      ],
      "metadata": {
        "id": "J5-p0w0UrUbk"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Paso 3: Procesamiento"
      ],
      "metadata": {
        "id": "hRR1sW5Le8E0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "G_t2zRnOMef3"
      },
      "outputs": [],
      "source": [
        "chatbot = ChatBotInmobiliaria()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "KmoBTlt2JfLD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3542befd-5f99-4cf7-97a0-baa23dcaad31"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:llama_index.llm_predictor.base:Unknown max input size for gpt-3.5-turbo, using defaults.\n"
          ]
        }
      ],
      "source": [
        "# CUIDADO !!\n",
        "#\n",
        "# Carga los datos del PDF y los guarda en un archivo JSON.\n",
        "# Si ya existe el archivo JSON, obviar este paso.\n",
        "#\n",
        "\n",
        "# chatbot.create_dataset(dataset_directory, DATASET_INMOBILIARIA_JSON)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "UKBH2pj0AgmP"
      },
      "outputs": [],
      "source": [
        "chatbot.load_dataset(DATASET_INMOBILIARIA_JSON)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Testing"
      ],
      "metadata": {
        "id": "LaxP4xMxfIyC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "juCQTxHMDMlf"
      },
      "outputs": [],
      "source": [
        "# Ejemplos:\n",
        "# ¿Cuánto es el precio promedio de una vivienda?\n",
        "# ¿Qué regulaciones tengo para comprar una vivienda?\n",
        "# Qué riesgos debo tener en cuenta para comprar una inmobiliaria?\n",
        "# qué influencia europea hay en la compra de una vivienda?\n",
        "\n",
        "while True:\n",
        "    pregunta = input('Escribe tu pregunta   \\n') + \"Responde en español\" \n",
        "    chatbot.ask(pregunta)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Interfaz Gradio"
      ],
      "metadata": {
        "id": "hgA4VV1yfXg_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Instalar librerías"
      ],
      "metadata": {
        "id": "vOYNd7jePmqy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gradio plotly scikit-learn PyPDF2 openai langchain llama-index==0.5.25"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C7GaCaoLfb-Y",
        "outputId": "d86bf530-ecfa-4daf-8c3b-1f0f5786cd89"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting gradio\n",
            "  Downloading gradio-3.29.0-py3-none-any.whl (17.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.3/17.3 MB\u001b[0m \u001b[31m40.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: plotly in /usr/local/lib/python3.10/dist-packages (5.13.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.2.2)\n",
            "Requirement already satisfied: PyPDF2 in /usr/local/lib/python3.10/dist-packages (3.0.1)\n",
            "Requirement already satisfied: openai in /usr/local/lib/python3.10/dist-packages (0.27.6)\n",
            "Requirement already satisfied: langchain in /usr/local/lib/python3.10/dist-packages (0.0.142)\n",
            "Requirement already satisfied: llama-index==0.5.25 in /usr/local/lib/python3.10/dist-packages (0.5.25)\n",
            "Requirement already satisfied: dataclasses-json in /usr/local/lib/python3.10/dist-packages (from llama-index==0.5.25) (0.5.7)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from llama-index==0.5.25) (1.22.4)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.2.0 in /usr/local/lib/python3.10/dist-packages (from llama-index==0.5.25) (8.2.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from llama-index==0.5.25) (1.5.3)\n",
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.10/dist-packages (from llama-index==0.5.25) (0.4.0)\n",
            "Requirement already satisfied: PyYAML>=5.4.1 in /usr/local/lib/python3.10/dist-packages (from langchain) (6.0)\n",
            "Requirement already satisfied: SQLAlchemy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.4.48)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.8.4)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.2)\n",
            "Requirement already satisfied: gptcache>=0.1.7 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.1.22)\n",
            "Requirement already satisfied: numexpr<3.0.0,>=2.8.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.8.4)\n",
            "Requirement already satisfied: openapi-schema-pydantic<2.0,>=1.2 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.2.4)\n",
            "Requirement already satisfied: pydantic<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.10.7)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.27.1)\n",
            "Collecting aiofiles (from gradio)\n",
            "  Downloading aiofiles-23.1.0-py3-none-any.whl (14 kB)\n",
            "Requirement already satisfied: altair>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (4.2.2)\n",
            "Collecting fastapi (from gradio)\n",
            "  Downloading fastapi-0.95.1-py3-none-any.whl (56 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.0/57.0 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting ffmpy (from gradio)\n",
            "  Downloading ffmpy-0.3.0.tar.gz (4.8 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting gradio-client>=0.2.1 (from gradio)\n",
            "  Downloading gradio_client-0.2.2-py3-none-any.whl (287 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m287.9/287.9 kB\u001b[0m \u001b[31m27.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting httpx (from gradio)\n",
            "  Downloading httpx-0.24.0-py3-none-any.whl (75 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.3/75.3 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting huggingface-hub>=0.13.0 (from gradio)\n",
            "  Downloading huggingface_hub-0.14.1-py3-none-any.whl (224 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m224.5/224.5 kB\u001b[0m \u001b[31m22.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.1.2)\n",
            "Requirement already satisfied: markdown-it-py[linkify]>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.2.0)\n",
            "Requirement already satisfied: markupsafe in /usr/local/lib/python3.10/dist-packages (from gradio) (2.1.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from gradio) (3.7.1)\n",
            "Collecting mdit-py-plugins<=0.3.3 (from gradio)\n",
            "  Downloading mdit_py_plugins-0.3.3-py3-none-any.whl (50 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.5/50.5 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting orjson (from gradio)\n",
            "  Downloading orjson-3.8.12-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (137 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m137.2/137.2 kB\u001b[0m \u001b[31m15.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (from gradio) (8.4.0)\n",
            "Collecting pydub (from gradio)\n",
            "  Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
            "Requirement already satisfied: pygments>=2.12.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.14.0)\n",
            "Collecting python-multipart (from gradio)\n",
            "  Downloading python_multipart-0.0.6-py3-none-any.whl (45 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.7/45.7 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting semantic-version (from gradio)\n",
            "  Downloading semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from gradio) (4.5.0)\n",
            "Collecting uvicorn>=0.14.0 (from gradio)\n",
            "  Downloading uvicorn-0.22.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting websockets>=10.0 (from gradio)\n",
            "  Downloading websockets-11.0.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (129 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.9/129.9 kB\u001b[0m \u001b[31m15.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.10.1)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.2.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.1.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai) (4.65.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (2.0.12)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
            "Requirement already satisfied: entrypoints in /usr/local/lib/python3.10/dist-packages (from altair>=4.2.0->gradio) (0.4)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.10/dist-packages (from altair>=4.2.0->gradio) (4.3.3)\n",
            "Requirement already satisfied: toolz in /usr/local/lib/python3.10/dist-packages (from altair>=4.2.0->gradio) (0.12.0)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.3.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json->llama-index==0.5.25) (3.19.0)\n",
            "Requirement already satisfied: marshmallow-enum<2.0.0,>=1.5.1 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json->llama-index==0.5.25) (1.5.1)\n",
            "Requirement already satisfied: typing-inspect>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json->llama-index==0.5.25) (0.8.0)\n",
            "Requirement already satisfied: cachetools in /usr/local/lib/python3.10/dist-packages (from gptcache>=0.1.7->langchain) (5.3.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from gradio-client>=0.2.1->gradio) (2023.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from gradio-client>=0.2.1->gradio) (23.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.13.0->gradio) (3.12.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py[linkify]>=2.0.0->gradio) (0.1.2)\n",
            "Collecting linkify-it-py<3,>=1 (from markdown-it-py[linkify]>=2.0.0->gradio)\n",
            "  Downloading linkify_it_py-2.0.2-py3-none-any.whl (19 kB)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->llama-index==0.5.25) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->llama-index==0.5.25) (2022.7.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2022.12.7)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.4)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<2,>=1->langchain) (2.0.2)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.10/dist-packages (from uvicorn>=0.14.0->gradio) (8.1.3)\n",
            "Collecting h11>=0.8 (from uvicorn>=0.14.0->gradio)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting starlette<0.27.0,>=0.26.1 (from fastapi->gradio)\n",
            "  Downloading starlette-0.26.1-py3-none-any.whl (66 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.9/66.9 kB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting httpcore<0.18.0,>=0.15.0 (from httpx->gradio)\n",
            "  Downloading httpcore-0.17.0-py3-none-any.whl (70 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m70.6/70.6 kB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx->gradio) (1.3.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->gradio) (1.0.7)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->gradio) (0.11.0)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->gradio) (4.39.3)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->gradio) (1.4.4)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->gradio) (3.0.9)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken->llama-index==0.5.25) (2022.10.31)\n",
            "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.10/dist-packages (from httpcore<0.18.0,>=0.15.0->httpx->gradio) (3.6.2)\n",
            "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair>=4.2.0->gradio) (0.19.3)\n",
            "Collecting uc-micro-py (from linkify-it-py<3,>=1->markdown-it-py[linkify]>=2.0.0->gradio)\n",
            "  Downloading uc_micro_py-1.0.2-py3-none-any.whl (6.2 kB)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->llama-index==0.5.25) (1.16.0)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from typing-inspect>=0.4.0->dataclasses-json->llama-index==0.5.25) (1.0.0)\n",
            "Building wheels for collected packages: ffmpy\n",
            "  Building wheel for ffmpy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ffmpy: filename=ffmpy-0.3.0-py3-none-any.whl size=4694 sha256=3457eefea7bcf5695df1803cbece3606aedeea6a0a79456e3e88d14be189a849\n",
            "  Stored in directory: /root/.cache/pip/wheels/0c/c2/0e/3b9c6845c6a4e35beb90910cc70d9ac9ab5d47402bd62af0df\n",
            "Successfully built ffmpy\n",
            "Installing collected packages: pydub, ffmpy, websockets, uc-micro-py, semantic-version, python-multipart, orjson, h11, aiofiles, uvicorn, starlette, mdit-py-plugins, linkify-it-py, huggingface-hub, httpcore, httpx, fastapi, gradio-client, gradio\n",
            "Successfully installed aiofiles-23.1.0 fastapi-0.95.1 ffmpy-0.3.0 gradio-3.29.0 gradio-client-0.2.2 h11-0.14.0 httpcore-0.17.0 httpx-0.24.0 huggingface-hub-0.14.1 linkify-it-py-2.0.2 mdit-py-plugins-0.3.3 orjson-3.8.12 pydub-0.25.1 python-multipart-0.0.6 semantic-version-2.10.0 starlette-0.26.1 uc-micro-py-1.0.2 uvicorn-0.22.0 websockets-11.0.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Interfaz"
      ],
      "metadata": {
        "id": "rVXLW_W_Ppza"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import gradio as gr\n",
        "from pathlib import Path\n",
        "import os\n",
        "import pandas as pd\n",
        "import time\n",
        "import random\n",
        "\n",
        "import openai\n",
        "from llama_index import GPTSimpleVectorIndex, SimpleDirectoryReader, LLMPredictor, ServiceContext\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "import textwrap\n",
        "\n",
        "# Procesar datos de PDF\n",
        "from langchain.document_loaders import PyPDFLoader\n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "\n",
        "#import gradio as gr\n",
        "from openai.embeddings_utils import get_embedding\n",
        "from openai.embeddings_utils import cosine_similarity\n",
        "\n",
        "\n",
        "# API KEY OPENAI\n",
        "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
        "os.environ[\"OPENAI_API_KEY\"] = OPENAI_API_KEY\n",
        "\n",
        "# CONSTANTES\n",
        "DATASET_JSON = \"demo-inmobiliaria.json\"\n",
        "\n",
        "# Ubicación dataset\n",
        "carpeta_actual = os.getcwd()\n",
        "print(f\"Nombre de la carpeta actual: {carpeta_actual}\")\n",
        "PATH_FILE = f\"{os.getcwd()}/{DATASET_JSON}\"\n",
        "\n",
        "class ChatBotInmobiliaria():\n",
        "    def __init__(self):\n",
        "        self.embedding_engine = \"text-embedding-ada-002\"\n",
        "        self.model_name = \"gpt-3.5-turbo\"\n",
        "        self.index = None\n",
        "    \n",
        "    def create_dataset(self, directory_path, filepath_dataset):\n",
        "        # directory_path: Directorio donde se ubican los archivos PDF.\n",
        "        # filepath_dataset: Nombre del archivo JSON vectorizado.\n",
        "        if directory_path != None:\n",
        "            #Leer los PDFs\n",
        "            pdf = SimpleDirectoryReader(directory_path).load_data()\n",
        "            #Definir e instanciar el modelo\n",
        "            modelo = LLMPredictor(llm=ChatOpenAI(temperature=0, model_name=self.model_name))\n",
        "            #Indexar el contenido de los PDFs\n",
        "            service_context = ServiceContext.from_defaults(llm_predictor=modelo)\n",
        "            self.index = GPTSimpleVectorIndex.from_documents(pdf, service_context = service_context)\n",
        "            self.__save_model(filepath_dataset)\n",
        "\n",
        "    def __save_model(self, filepath):\n",
        "        #Guardar el índice a disco para no tener que repetir cada vez\n",
        "        #Recordar que necesistaríamos persistir el drive para que lo mantenga\n",
        "        self.index.save_to_disk(filepath)\n",
        "    \n",
        "    def load_dataset(self, filepath):\n",
        "        #Cargar el índice del disco\n",
        "        self.index = GPTSimpleVectorIndex.load_from_disk(filepath)\n",
        "\n",
        "    def ask(self, question=\"\"):\n",
        "        if len(question) == 0:\n",
        "            print(\"Debe de ingresar una pregunta.\")\n",
        "        try:\n",
        "            return self.index.query(question + \"\\nResponde en español\")\n",
        "        except Exception as e:\n",
        "            print(e)\n",
        "            return \"Hubo un error.\"\n",
        "    \n",
        "\n",
        "# Gradio\n",
        "\n",
        "description =\"\"\"\n",
        "<p>\n",
        "<center>\n",
        "Demo Inmobiliaria, el objetivo es responder preguntas a través de OpenAI previamente entrenado con un archivo PDF.\n",
        "<img src=\"https://raw.githubusercontent.com/All-Aideas/sea_apirest/main/logo.png\" alt=\"logo\" width=\"250\"/>\n",
        "</center>\n",
        "</p>\n",
        "\"\"\"\n",
        "\n",
        "article = \"<p style='text-align: center'><a href='http://allaideas.com/index.html' target='_blank'>Demo Inmobiliaria: Link para más info</a> </p>\"\n",
        "examples = [[\"¿Cuánto está una casa en San Isidro?\"],[\"Hay precios más baratos?\"],[\"A dónde llamo?\", \"Qué leyes existen?\"]]\n",
        "\n",
        "gpt_bot = ChatBotInmobiliaria()\n",
        "gpt_bot.load_dataset(PATH_FILE)\n",
        "chat_history = []\n",
        "\n",
        "def chat(pregunta):\n",
        "    bot_message = str(gpt_bot.ask(question=pregunta))\n",
        "    chat_history.append((pregunta, bot_message))\n",
        "    time.sleep(1)\n",
        "    return chat_history\n",
        "\n",
        "in1 = gr.inputs.Textbox(label=\"Pregunta\")\n",
        "out1 = gr.outputs.Chatbot(label=\"Respuesta\").style(height=350)\n",
        "\n",
        "demo = gr.Interface(\n",
        "    fn=chat,\n",
        "    inputs=in1,\n",
        "    outputs=out1,\n",
        "    title=\"Demo Inmobiliaria\",\n",
        "    description=description,\n",
        "    article=article,\n",
        "    enable_queue=True,\n",
        "    examples=examples,\n",
        "    )\n",
        "\n",
        "demo.launch(debug=True)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 844
        },
        "id": "GgYe4bKOgwtR",
        "outputId": "0de83754-d166-4385-ee68-ce38137935ec"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Nombre de la carpeta actual: /content\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gradio/inputs.py:27: UserWarning: Usage of gradio.inputs is deprecated, and will not be supported in the future, please import your component from gradio.components\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/gradio/deprecation.py:40: UserWarning: `optional` parameter is deprecated, and it has no effect\n",
            "  warnings.warn(value)\n",
            "/usr/local/lib/python3.10/dist-packages/gradio/deprecation.py:40: UserWarning: `numeric` parameter is deprecated, and it has no effect\n",
            "  warnings.warn(value)\n",
            "/usr/local/lib/python3.10/dist-packages/gradio/outputs.py:309: UserWarning: Usage of gradio.outputs is deprecated, and will not be supported in the future, please import your components from gradio.components\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/gradio/deprecation.py:40: UserWarning: `enable_queue` is deprecated in `Interface()`, please use it within `launch()` instead.\n",
            "  warnings.warn(value)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "Note: opening Chrome Inspector may crash demo inside Colab notebooks.\n",
            "\n",
            "To create a public link, set `share=True` in `launch()`.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "(async (port, path, width, height, cache, element) => {\n",
              "                        if (!google.colab.kernel.accessAllowed && !cache) {\n",
              "                            return;\n",
              "                        }\n",
              "                        element.appendChild(document.createTextNode(''));\n",
              "                        const url = await google.colab.kernel.proxyPort(port, {cache});\n",
              "\n",
              "                        const external_link = document.createElement('div');\n",
              "                        external_link.innerHTML = `\n",
              "                            <div style=\"font-family: monospace; margin-bottom: 0.5rem\">\n",
              "                                Running on <a href=${new URL(path, url).toString()} target=\"_blank\">\n",
              "                                    https://localhost:${port}${path}\n",
              "                                </a>\n",
              "                            </div>\n",
              "                        `;\n",
              "                        element.appendChild(external_link);\n",
              "\n",
              "                        const iframe = document.createElement('iframe');\n",
              "                        iframe.src = new URL(path, url).toString();\n",
              "                        iframe.height = height;\n",
              "                        iframe.allow = \"autoplay; camera; microphone; clipboard-read; clipboard-write;\"\n",
              "                        iframe.width = width;\n",
              "                        iframe.style.border = 0;\n",
              "                        element.appendChild(iframe);\n",
              "                    })(7860, \"/\", \"100%\", 500, false, window.element)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Keyboard interruption in main thread... closing server.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}